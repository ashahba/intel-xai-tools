import os
from pathlib import Path
import json
import pandas as pd
import numpy as np
from transformers import AutoModelForSequenceClassification, DistilBertTokenizerFast, pipeline, AutoTokenizer
from optimum.habana import GaudiTrainer, GaudiTrainingArguments
import evaluate
import torch
from torch.utils.data import Dataset
from torch.nn.functional import softmax
import argparse

os.environ["TOKENIZERS_PARALLELISM"] = "true"

def parse_args():

    parser = argparse.ArgumentParser(
        prog='Test',
        description='Test checkpoint generated by train.py with jigsaw or tc from checkpoint. \
             Can also test CitizenLab model.',
        epilog='WIP'
    )

    parser.add_argument('-m', '--model-path', help='Path to the model checkpoint that \
                        will be tested.')
    parser.add_argument('-d', '--dataset-name', help='ToxicChat=\'tc\' or \
                        Jigsaw Unintended Bias=\'jigsaw\'')
    parser.add_argument('-r', '--results-path', help='Optional. Only set results path if you are \
                        testing model from hugging face hub.')
    return parser.parse_args()

class BertoxDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

def validate_metrics(predict_results):
    roc_auc = load_metric("roc_auc")
    n_samples = len(predict_results.label_ids)
    probabilities = softmax(torch.Tensor(predict_results.predictions))[:,1]
    auroc = roc_auc.compute(prediction_scores=probabilities, references=predict_results.label_ids)
    preds = np.argmax(predict_results.predictions, axis=-1)

    n_correct = 0
    for i in range(n_samples):
        if preds[i] == predict_results.label_ids[i]:
            n_correct += 1 
    accuracy = n_correct/n_samples 

    print(f'My accuracy: {accuracy}.\nEvaluate accuracy: {predict_results.metrics["test_accuracy"]} ')
    print(f'My auroc: {auroc}.\nEvaluate accuracy: {predict_results.metrics["test_auroc"]} ')

def read_test_tc_split(csv_path, CL):
    df = pd.read_csv(csv_path)
    texts = list(df.user_input)
    labels = list(df.toxicity) 

    # if we're testing Citizen Lab, switch labels
    # to match config of CL model where 
    # toxic=0 and not_toxic=1
    if CL:
        swap = {0:1, 1:0}
        labels = [swap[i] for i in labels]
    
    return texts, labels

def generate_datasets(test_texts, test_labels, tokenizer):
    test_encodings = tokenizer(test_texts, truncation=True, padding=True)
    test_dataset = BertoxDataset(test_encodings, test_labels)

    return test_dataset

print('loading accuracy metric')
accuracy = evaluate.load("accuracy")
print('loading auroc metric')
roc_auc = evaluate.load("roc_auc")
print('loading f1 metric')
f1_metric = evaluate.load("f1")
print('loading precision metric')
precision_metric = evaluate.load("precision")
print('loading recall metric')
recall_metric = evaluate.load("recall")

def compute_metrics(eval_pred):

    logits, labels = eval_pred
    probabilities = softmax(torch.Tensor(logits))[:,1]
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy.compute(predictions=predictions, references=labels)
    auroc = roc_auc.compute(prediction_scores=probabilities, references=labels)
    f1 = f1_metric.compute(predictions=predictions, references=labels)
    precision = precision_metric.compute(predictions=predictions, references=labels)
    recall = recall_metric.compute(predictions=predictions, references=labels)
    
    return {'accuracy': acc['accuracy'], 'auroc': auroc['roc_auc'], 'f1': f1['f1'], 'precision': precision['precision'], 'recall': recall['recall']}


def save_predictions(prediction_results, input_texts, results_path, CL):
    
    if CL: 
        results_df = pd.DataFrame({
            'input': input_texts,
            'target': prediction_results.label_ids,
            'toxic': prediction_results.predictions[:,0],
            'not_toxic': prediction_results.predictions[:,1],
        })
    else:
        results_df = pd.DataFrame({
            'input': input_texts,
            'target': prediction_results.label_ids,
            'not_toxic': prediction_results.predictions[:,0],
            'toxic': prediction_results.predictions[:,1],
        })

    results_df.to_csv(results_path)
    return results_df


def main():
    args = parse_args()
    print(args)
    CL = False
    if 'citizenlab' in args.model_path:
        CL = True

    CHECKPOINT_NAME = os.path.basename(args.model_path)
    if CL: 
        WORKFLOW_DIR = Path(args.model_path).parent.absolute()
    elif args.results_path is None:
        WORKFLOW_DIR = Path(args.model_path).parent.absolute().parent.absolute()
    else:
        WORKFLOW_DIR = args.results_path
    

    TEST_RESULTS_PATH = os.path.join(WORKFLOW_DIR, 'results', f'{CHECKPOINT_NAME}_{args.dataset_name}_accuracy')
    print(f'Saving results in {TEST_RESULTS_PATH}')

    if not os.path.exists(TEST_RESULTS_PATH):
        os.makedirs(TEST_RESULTS_PATH)
    
    if args.dataset_name == 'tc':
        DATA_PATH = 'hf://datasets/lmsys/toxic-chat/data/0124/toxic-chat_annotation_test.csv'
        test_texts, test_labels = read_test_tc_split(DATA_PATH, CL)
    else:
        print(f'Support for dataset is coming soon...')
    


    model = AutoModelForSequenceClassification.from_pretrained(args.model_path)
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)

    test_dataset = generate_datasets(test_texts, test_labels, tokenizer)
    training_args = GaudiTrainingArguments(output_dir=TEST_RESULTS_PATH,
        use_habana=True,
        use_lazy_mode=True,
        gaudi_config_name='Habana/roberta-base',
    )

    trainer = GaudiTrainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        eval_dataset=test_dataset,           # evaluation dataset
        compute_metrics=compute_metrics
    )

    results = trainer.predict(test_dataset)

    save_predictions(results, test_texts, os.path.join(TEST_RESULTS_PATH,'predictions.csv'), CL)
    json.dump(results.metrics, open(os.path.join(TEST_RESULTS_PATH,'metrics.json'), 'w'))

if __name__ == '__main__':
    main()